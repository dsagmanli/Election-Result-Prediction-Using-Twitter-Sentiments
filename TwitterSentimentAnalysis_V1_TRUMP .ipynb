{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "914187a5-67f8-4f13-b303-0d10ff809c77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Unveiling Twitter Sentiments: A Big Data Dive into Twitter Sentiments during the 2020 US Elections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42831ab9-250f-4342-a694-385079cba219",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This notebook contains code for the Twitter Sentiment Analysis for Trump DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b834ee89-a304-4578-b587-04b268bc8b62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c5897cd-08dd-4827-b098-f7e0ec8ed0e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Installing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21afad94-6fd8-4507-8ba2-6b63d4617f01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting wordninja\n  Downloading wordninja-2.0.0.tar.gz (541 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 541.6/541.6 kB 12.6 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nBuilding wheels for collected packages: wordninja\n  Building wheel for wordninja (setup.py): started\n  Building wheel for wordninja (setup.py): finished with status 'done'\n  Created wheel for wordninja: filename=wordninja-2.0.0-py3-none-any.whl size=541530 sha256=15692341d3aad574cce66948af41333b764c89185f68b115e3cc186397f14566\n  Stored in directory: /home/spark-462fc5f1-0c86-49a9-a399-33/.cache/pip/wheels/7c/e6/e6/e95742bec8d8c3d40687c0c50b8537bb71347ce84a2b322234\nSuccessfully built wordninja\nInstalling collected packages: wordninja\nSuccessfully installed wordninja-2.0.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting langdetect\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 981.5/981.5 kB 20.9 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nRequirement already satisfied: six in /usr/lib/python3/dist-packages (from langdetect) (1.16.0)\nBuilding wheels for collected packages: langdetect\n  Building wheel for langdetect (setup.py): started\n  Building wheel for langdetect (setup.py): finished with status 'done'\n  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=5626c3da5d2e70709c599d8408fcdd8b20143fcc272d968ca48f7b059cd549a5\n  Stored in directory: /home/spark-462fc5f1-0c86-49a9-a399-33/.cache/pip/wheels/c4/16/af/1889804d8b7c0c041cadee8e29673a938a332acbf2865c70a1\nSuccessfully built langdetect\nInstalling collected packages: langdetect\nSuccessfully installed langdetect-1.0.9\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting textblob\n  Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 626.3/626.3 kB 12.5 MB/s eta 0:00:00\nCollecting nltk>=3.8\n  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 42.4 MB/s eta 0:00:00\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.10/site-packages (from nltk>=3.8->textblob) (1.2.0)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.10/site-packages (from nltk>=3.8->textblob) (4.64.1)\nRequirement already satisfied: click in /databricks/python3/lib/python3.10/site-packages (from nltk>=3.8->textblob) (8.0.4)\nRequirement already satisfied: regex>=2021.8.3 in /databricks/python3/lib/python3.10/site-packages (from nltk>=3.8->textblob) (2022.7.9)\nInstalling collected packages: nltk, textblob\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.7\n    Not uninstalling nltk at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-462fc5f1-0c86-49a9-a399-3300284b7fcf\n    Can't uninstall 'nltk'. No files were found to uninstall.\nSuccessfully installed nltk-3.8.1 textblob-0.18.0.post0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "#Installing necessary libraries \n",
    "%pip install wordninja\n",
    "\n",
    "%pip install langdetect\n",
    "\n",
    "%pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce737360-b7d3-4a70-a44b-9dfc12c101b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Initiating Sprak Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21cdf26b-3117-485a-8e24-9ddd409e1139",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Twitter_Sentiment_Analysis\") \\\n",
    "    .appName(\"Group_Project\") \\\n",
    "    .appName(\"Trump_V1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b48872a-84e3-4867-aef5-477c9e09aa06",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected internal error when monkey patching sc._repr_html_: An error occurred while calling z:com.databricks.backend.daemon.driver.DriverLocal.commandContext. Trace:\npy4j.security.Py4JSecurityException: Method public static java.lang.ThreadLocal com.databricks.backend.daemon.driver.DriverLocal.commandContext() is not whitelisted on class class com.databricks.backend.daemon.driver.DriverLocal\n\tat py4j.security.WhitelistingPy4JSecurityManager.checkCall(WhitelistingPy4JSecurityManager.java:473)\n\tat py4j.Gateway.invoke(Gateway.java:305)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.139.64.10:40001\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.139.64.10:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://10.139.64.10:7077 appName=Databricks Shell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c9267ae-4a0c-4799-9ad9-3a4fb7b909ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aab2724d-b517-4514-b744-83a974a074c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import regexp_extract, lit, when, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import length, split, size, col, avg, regexp_replace\n",
    "from pyspark.sql.functions import col, trim\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import split, col, concat_ws\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import wordcloud\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType, FloatType\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# To ensure that each column's content is fully displayed without being truncated, we have set: \n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a89260a3-0d47-409f-948d-c8e4851612d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Setting up Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae54adfb-4021-4146-8039-083b8af8eb77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (2310699, 21)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Defining my custom schema since we are not doing inferSchema= True, since it will take too much time \n",
    "custom_schema = StructType([\n",
    "    StructField(\"created_at\", StringType(), True),\n",
    "    StructField(\"tweet_id\", StringType(), True),\n",
    "    StructField(\"tweet\", StringType(), True),\n",
    "    StructField(\"likes\", StringType(), True),\n",
    "    StructField(\"retweet_count\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"user_name\", StringType(), True),\n",
    "    StructField(\"user_screen_name\", StringType(), True),\n",
    "    StructField(\"user_description\", StringType(), True),\n",
    "    StructField(\"user_join_date\", StringType(), True),\n",
    "    StructField(\"user_followers_count\", StringType(), True),\n",
    "    StructField(\"user_location\", StringType(), True),\n",
    "    StructField(\"lat\", StringType(), True),\n",
    "    StructField(\"long\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"continent\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"state_code\", StringType(), True),\n",
    "    StructField(\"collected_at\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Reading the CSV file with custom schema\n",
    "df_t= spark.read.csv(\"/FileStore/tables/hashtag_donaldtrump-2.csv\", schema=custom_schema)\n",
    "\n",
    "#persisitng the dataframe to store it in the memory until performing all the pre-processing then finally caching it \n",
    "df_t.persist()\n",
    "#running a count calculation to check how many rows and columns are present in the dataframe\n",
    "num_rows = df_t.count()\n",
    "num_columns = len(df_t.columns)\n",
    "print(\"DataFrame shape:\", (num_rows, num_columns)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2dfce96-23da-4f6f-938d-1cecc401e64b",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-------------+------------------+--------------------+--------------------+-------------------+--------------------+-------------------+--------------------+-------------------+------------------+--------------------+-------------+--------------------+-------------+--------------------+----------+--------------------+\n|          created_at|            tweet_id|               tweet|               likes|retweet_count|            source|             user_id|           user_name|   user_screen_name|    user_description|     user_join_date|user_followers_count|      user_location|               lat|                long|         city|             country|    continent|               state|state_code|        collected_at|\n+--------------------+--------------------+--------------------+--------------------+-------------+------------------+--------------------+--------------------+-------------------+--------------------+-------------------+--------------------+-------------------+------------------+--------------------+-------------+--------------------+-------------+--------------------+----------+--------------------+\n|          created_at|            tweet_id|               tweet|               likes|retweet_count|            source|             user_id|           user_name|   user_screen_name|    user_description|     user_join_date|user_followers_count|      user_location|               lat|                long|         city|             country|    continent|               state|state_code|        collected_at|\n| 2020-10-15 00:00:01|1.316529221557252...|#Elecciones2020 |...|                NULL|         NULL|              NULL|                NULL|                NULL|               NULL|                NULL|               NULL|                NULL|               NULL|              NULL|                NULL|         NULL|                NULL|         NULL|                NULL|      NULL|                NULL|\n|                   ⠀|                NULL|                NULL|                NULL|         NULL|              NULL|                NULL|                NULL|               NULL|                NULL|               NULL|                NULL|               NULL|              NULL|                NULL|         NULL|                NULL|         NULL|                NULL|      NULL|                NULL|\n|🌐https://t.co/qh...|                NULL|                NULL|                NULL|         NULL|              NULL|                NULL|                NULL|               NULL|                NULL|               NULL|                NULL|               NULL|              NULL|                NULL|         NULL|                NULL|         NULL|                NULL|      NULL|                NULL|\n|                   _|                NULL|                NULL|                NULL|         NULL|              NULL|                NULL|                NULL|               NULL|                NULL|               NULL|                NULL|               NULL|              NULL|                NULL|         NULL|                NULL|         NULL|                NULL|      NULL|                NULL|\n|#ElSolLatino #yob...|                 0.0|                 0.0|           TweetDeck|  360666534.0|El Sol Latino News|     elsollatinonews|🌐 Noticias de in...|               NULL|                NULL|               NULL|                NULL|               NULL|              NULL|                NULL|         NULL|                NULL|         NULL|                NULL|      NULL|                NULL|\n|⠀⏹️ Facebook e In...|                NULL|                NULL|                NULL|         NULL|              NULL|                NULL|                NULL|               NULL|                NULL|               NULL|                NULL|               NULL|              NULL|                NULL|         NULL|                NULL|         NULL|                NULL|      NULL|                NULL|\n|⠀🏙️ Philadelphia...|                NULL|                NULL|                NULL|         NULL|              NULL|                NULL|                NULL|               NULL|                NULL|               NULL|                NULL|               NULL|              NULL|                NULL|         NULL|                NULL|         NULL|                NULL|      NULL|                NULL|\n|⠀🌅 Miami: /elsol...| 2011-08-23 15:33:45|              1860.0|Philadelphia, PA ...|     25.77427|         -80.19366|                NULL|United States of ...|      North America|             Florida|                 FL| 2020-10-21 00:00:00|               NULL|              NULL|                NULL|         NULL|                NULL|         NULL|                NULL|      NULL|                NULL|\n| 2020-10-15 00:00:01|1.316529222748430...|Usa 2020, Trump c...|                26.0|          9.0|  Social Mediaset |         331617619.0|             Tgcom24|    MediasetTgcom24|Profilo ufficiale...|2011-07-08 13:12:20|           1067661.0|               NULL|              NULL|                NULL|         NULL|                NULL|         NULL|                NULL|      NULL|2020-10-21 00:00:...|\n| 2020-10-15 00:00:02|1.316529228091847...|#Trump: As a stud...|                NULL|         NULL|              NULL|                NULL|                NULL|               NULL|                NULL|               NULL|                NULL|               NULL|              NULL|                NULL|         NULL|                NULL|         NULL|                NULL|      NULL|                NULL|\n|          Um. What?\"|                 2.0|                 1.0|     Twitter Web App|    8436472.0|            snarke|              snarke|Will mock for foo...|2007-08-26 05:56:11|              1185.0|           Portland|          45.5202471|       -122.6741949|          Portland|United States of ...|North America|              Oregon|           OR|2020-10-21 00:00:...|      NULL|                NULL|\n| 2020-10-15 00:00:02|1.316529227471237...|2 hours since las...|                 0.0|          0.0|     Trumpytweeter|8.28355589206057e+17|       Trumpytweeter|      trumpytweeter|If he doesn't twe...|2017-02-05 21:32:17|                32.0|               NULL|              NULL|                NULL|         NULL|                NULL|         NULL|                NULL|      NULL|2020-10-21 00:00:...|\n| 2020-10-15 00:00:08|1.316529252301451...|You get a tie! An...|                 4.0|          3.0|Twitter for iPhone|          47413798.0|Rana Abtar - رنا ...|          Ranaabtar|Washington Corres...|2009-06-15 19:05:35|              5393.0|      Washington DC|        38.8949924|         -77.0365581|   Washington|United States of ...|North America|District of Columbia|        DC|2020-10-21 00:00:...|\n| 2020-10-15 00:00:17|1.316529291052675...|@CLady62 Her 15 m...|                NULL|         NULL|              NULL|                NULL|                NULL|               NULL|                NULL|               NULL|                NULL|               NULL|              NULL|                NULL|         NULL|                NULL|         NULL|                NULL|      NULL|                NULL|\n|She cried to #Tru...|                 2.0|                 0.0| Twitter for Android| 1138416104.0|      Farris Flagg|         FarrisFlagg|#BidenHarris2020 ...|2013-02-01 01:37:38|              2363.0|  Perris,California|          33.7825194|-117.22864779999999|              NULL|United States of ...|North America|          California|           CA|2020-10-21 00:00:...|      NULL|                NULL|\n| 2020-10-15 00:00:17|1.316529289949569...|@richardmarx Glad...|                 0.0|          0.0|Twitter for iPhone|7.674018410302095...|      Michael Wilson|        wilsonfire9|                NULL|2016-08-21 16:43:51|                75.0|         Powell, TN|              NULL|                NULL|         NULL|                NULL|         NULL|                NULL|      NULL|2020-10-21 00:00:...|\n| 2020-10-15 00:00:18|1.316529293497962...|@DeeviousDenise @...|                 0.0|          0.0|Twitter for iPhone|9.007610716314296...|Stacey Gulledge ?...|        sm_gulledge|Patriot, Wife, “S...|2017-08-24 16:45:49|               766.0|          Ohio, USA|40.225356899999994|         -82.6881395|         NULL|United States of ...|North America|                Ohio|        OH|2020-10-21 00:00:...|\n| 2020-10-15 00:00:20|1.316529301332918...|One of the single...|                 0.0|          0.0|   Twitter Web App|         540476889.0|              Jamieo|           jamieo33|Don't know what I...|2012-03-30 00:30:54|               151.0|  Pennsylvania, USA|        40.9699889|  -77.72788309999999|         NULL|United States of ...|North America|        Pennsylvania|        PA|2020-10-21 00:00:...|\n| 2020-10-15 00:00:21|1.316529308576309...|#Election2020 #Tr...|                NULL|         NULL|              NULL|                NULL|                NULL|               NULL|                NULL|               NULL|                NULL|               NULL|              NULL|                NULL|         NULL|                NULL|         NULL|                NULL|      NULL|                NULL|\n+--------------------+--------------------+--------------------+--------------------+-------------+------------------+--------------------+--------------------+-------------------+--------------------+-------------------+--------------------+-------------------+------------------+--------------------+-------------+--------------------+-------------+--------------------+----------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14f3413b-61f8-4ab8-98e6-eaab46c347dc",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- created_at: string (nullable = true)\n |-- tweet_id: string (nullable = true)\n |-- tweet: string (nullable = true)\n |-- likes: string (nullable = true)\n |-- retweet_count: string (nullable = true)\n |-- source: string (nullable = true)\n |-- user_id: string (nullable = true)\n |-- user_name: string (nullable = true)\n |-- user_screen_name: string (nullable = true)\n |-- user_description: string (nullable = true)\n |-- user_join_date: string (nullable = true)\n |-- user_followers_count: string (nullable = true)\n |-- user_location: string (nullable = true)\n |-- lat: string (nullable = true)\n |-- long: string (nullable = true)\n |-- city: string (nullable = true)\n |-- country: string (nullable = true)\n |-- continent: string (nullable = true)\n |-- state: string (nullable = true)\n |-- state_code: string (nullable = true)\n |-- collected_at: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#printing the schema to check for the datatypes and everything else \n",
    "df_t.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1693aa6-bb61-4875-aee7-9348976daaa4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_t= df_t.distinct() #we assumed that just because somebody retweets something doesn't mean they agree with it so we got rid of retweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d0964c-956c-47d1-8fca-daaba77d3356",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Column details\n",
    "    created_at: Date and time of tweet creation\n",
    "    tweet_id: Unique ID of the tweet\n",
    "    tweet: Full tweet text \n",
    "    likes: Number of likes \n",
    "    retweet_count: Number of retweets\n",
    "    source: Utility used to post tweet\n",
    "    user_id: User ID of tweet creator\n",
    "    user_name: Username of tweet creator\n",
    "    user_screen_name: Screen name of tweet creator\n",
    "    user_description: Description of self by tweet creator\n",
    "    user_join_date: Join date of tweet creator\n",
    "    user_followers_count: Followers count on tweet creator\n",
    "    user_location: Location given on tweet creator's profile\n",
    "    lat: Latitude parsed from user_location\n",
    "    long: Longitude parsed from user_location\n",
    "    city: City parsed from user_location\n",
    "    country: Country parsed from user_location\n",
    "    state: State parsed from user_location\n",
    "    state_code: State code parsed from user_location\n",
    "    collected_at: Date and time tweet data was mined from twitter*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d9eab27-6b31-4c40-9038-55d9f66c2871",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cba77a0-1e8b-42be-8c9c-282a3502d80d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####  For Starters we are validating the datetime format of the tweets in the 'created_at' column and then if the format is wrong correcting them and then creating two new columns 'new_created_at' and 'new_tweet' to store the correct formatted tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab4f3c0c-c239-41f3-bacf-f79a8f12059a",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|new_created_at     |new_tweet                                                                                                                                                                                                                                                        |\n+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|NULL               |The Revolutionary Roots of America's Religious Nationalism https://t.co/OuRIUV4Kvy                                                                                                                                                                               |\n|2020-10-19 00:47:33|@DineshDSouza @realDonaldTrump Omg- give me a break. The #Trump family should in jail.                                                                                                                                                                           |\n|NULL               |#JoeBiden #TheBigGuy\"                                                                                                                                                                                                                                            |\n|2020-10-19 00:47:45|@MSNBC .@MSNBC \\n                                                                                                                                                                                                                                                |\n|NULL               |#GreatAwakening #WalkAway #BLexit #Magick spells like this don't work against #Prayer https://t.co/ffKREUp3qy\"                                                                                                                                                   |\n|NULL               |Former officials in the #DonaldTrump administration share what they know. https://t.co/vOtM8pLIgh\"                                                                                                                                                               |\n|NULL               |Someone needs to answer the real question everyone wants to know. No one recovers from the #Coronavirus this fast like #DonaldTrump . Is this the real #DonaldTrump ? Why did he say he will leave the US if he's not elected president? https://t.co/sWBEB484uP\"|\n|NULL               |#HuntersLapTop #HunterBidenEmails                                                                                                                                                                                                                                |\n|NULL               |All investment information is my opinion. No investment advise offered.\"                                                                                                                                                                                         |\n|2020-10-19 00:47:41|Starting to think #HRC and the #Dems just erased #HunterBiden name off the #SteeleDossier and inserted #Trump \\n                                                                                                                                                 |\n|NULL               |In public I don't(99%) let anyone masked come within 6ft/2m of me unless there's a barrier between us. Don't wanna die from anyone's virus carrying! https://t.co/W8qa5B9fr2\"                                                                                    |\n|2020-10-19 00:47:41|\"THIS AIRS AT 9 PM ET TONIGHT ON #CNN. \"\"The Insiders.\"\"                                                                                                                                                                                                         |\n|NULL               |#WakeUpAmerica\"                                                                                                                                                                                                                                                  |\n|NULL               |If Hunters laptop has a pee pee tape on it I will lose my sh!t 😂                                                                                                                                                                                                |\n|NULL               |#Trump2020                                                                                                                                                                                                                                                       |\n|NULL               |#trump\"                                                                                                                                                                                                                                                          |\n|2020-10-19 00:47:35|Looks like the Governor of #Michigan has figured out a way to solve the #Trump problem and is calling on the #Communist #Chinese forces to strike a blow for #Slavery and Rule by the #Elite.\\n                                                                  |\n|2020-10-19 00:47:35|@carmenmom1209 @realDonaldTrump Oh what will they do??  Wonder if they will all be in the same prison??\\n                                                                                                                                                        |\n|2020-10-19 00:47:38|@Carrie71237358 @Regan0002112 @aubrey_huff Oh masks don't work, and Flu shots deadly, what are you going to do when #Trump Bail jumps to #Russia ? Sell everything and follow him like the people did with Jim Jones? https://t.co/pQ3NxU9zHD                    |\n|NULL               |HAHAHA!                                                                                                                                                                                                                                                          |\n+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming valid datetime entries in 'created_at' are in the format \"YYYY-MM-DD HH:MM:SS\"\n",
    "datetime_pattern = r\"^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}$\"\n",
    "\n",
    "# Adjusting the  columns based on datetime validation\n",
    "df_t = df_t.withColumn(\n",
    "    \"new_created_at\",\n",
    "    when(regexp_extract(col(\"created_at\"), datetime_pattern, 0) != \"\", col(\"created_at\")).otherwise(lit(None))\n",
    ")\n",
    "\n",
    "df_t = df_t.withColumn(\n",
    "    \"new_tweet\",\n",
    "    when(regexp_extract(col(\"created_at\"), datetime_pattern, 0) == \"\", col(\"created_at\")).otherwise(col(\"tweet\"))\n",
    ")\n",
    "df_t.select(\"new_created_at\", \"new_tweet\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1652b65-53a8-44e1-9273-c8f9d4b92ef2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Dropping Coloumns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a550da99-78e1-4a74-ba85-a34c5c946f82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- tweet_id: string (nullable = true)\n |-- likes: string (nullable = true)\n |-- retweet_count: string (nullable = true)\n |-- source: string (nullable = true)\n |-- user_id: string (nullable = true)\n |-- user_name: string (nullable = true)\n |-- user_screen_name: string (nullable = true)\n |-- user_description: string (nullable = true)\n |-- user_join_date: string (nullable = true)\n |-- user_followers_count: string (nullable = true)\n |-- user_location: string (nullable = true)\n |-- lat: string (nullable = true)\n |-- long: string (nullable = true)\n |-- city: string (nullable = true)\n |-- country: string (nullable = true)\n |-- continent: string (nullable = true)\n |-- state: string (nullable = true)\n |-- state_code: string (nullable = true)\n |-- collected_at: string (nullable = true)\n |-- new_created_at: string (nullable = true)\n |-- new_tweet: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#now dropping the old 'created_at' and 'tweet' columns since we dont need them anymore \n",
    "df_t = df_t.drop('created_at', 'tweet') \n",
    "#printing schema to check whether above columns have been dropped or not\n",
    "df_t.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de88912c-9bfc-48c4-a19b-e2950cbacfcf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### There were empty tweets and they had to be removed since they dont provide any information to us "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05434d9e-cef3-460d-9af7-b4c282092bb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter out rows where 'tweet' is None or empty (including rows that only have whitespace)\n",
    "df_t = df_t.filter((col(\"new_tweet\").isNotNull()) & (trim(col(\"new_tweet\")) != \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c05f0a47-aa6d-4984-8cc1-9d7cdb3720ad",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+--------------------+---------------+--------------------+-------------------+--------------------+--------------------+--------------------+-------------------+-----------+--------------------+-------------+----------------+---------+--------------------+----------+--------------------+-------------------+--------------------+\n|            tweet_id|              likes|       retweet_count|              source|        user_id|           user_name|   user_screen_name|    user_description|      user_join_date|user_followers_count|      user_location|        lat|                long|         city|         country|continent|               state|state_code|        collected_at|     new_created_at|           new_tweet|\n+--------------------+-------------------+--------------------+--------------------+---------------+--------------------+-------------------+--------------------+--------------------+--------------------+-------------------+-----------+--------------------+-------------+----------------+---------+--------------------+----------+--------------------+-------------------+--------------------+\n|                NULL|               NULL|                NULL|                NULL|           NULL|                NULL|               NULL|                NULL|                NULL|                NULL|               NULL|       NULL|                NULL|         NULL|            NULL|     NULL|                NULL|      NULL|                NULL|               NULL|The Revolutionary...|\n|1.317990737678618...|                0.0|                 0.0|  Twitter for iPhone|   2989226131.0|        CircleofLife|    soakupthesun121|                NULL| 2015-01-19 02:24:51|                40.0|               NULL|       NULL|                NULL|         NULL|            NULL|     NULL|                NULL|      NULL|2020-10-21 08:14:...|2020-10-19 00:47:33|@DineshDSouza @re...|\n|                 0.0|    Twitter Web App|1.040728819211100...|             The Ric|   RicAnderson7|    ALL LIVES MATTER|2018-09-14 22:27:41|              1305.0|        Low Country |                NULL|               NULL|       NULL|                NULL|         NULL|            NULL|     NULL|2020-10-21 08:14:...|      NULL|                NULL|               NULL|#JoeBiden #TheBig...|\n|1.317990786231828...|               NULL|                NULL|                NULL|           NULL|                NULL|               NULL|                NULL|                NULL|                NULL|               NULL|       NULL|                NULL|         NULL|            NULL|     NULL|                NULL|      NULL|                NULL|2020-10-19 00:47:45|   @MSNBC .@MSNBC \\n|\n|                 0.0|    Twitter Web App|         257097577.0|       Citizen of EU|     wavetossed|An EU citizen who...|2011-02-24 18:35:34|              5689.0|       Vancouver, BC|          49.2608724|-123.11395290000002|  Vancouver|              Canada|North America|British Columbia|       BC|2020-10-21 08:14:...|      NULL|                NULL|               NULL|#GreatAwakening #...|\n|                 7.0|    Twitter Web App|          84124051.0|Antifa Aspie 🌈/ ...|MikeFreethinker|🌈#Aspie The Empe...|2009-10-21 17:52:42|              1651.0|       United States|          39.7837304|       -100.4458825|       NULL|       United States|North America|            NULL|     NULL|2020-10-21 08:14:...|      NULL|                NULL|               NULL|Former officials ...|\n|                 0.0|    Twitter Web App|8.289821571671654...|Trumpdumpcare #Tr...|  trumpdumpcare|If 100,000 dead b...|               NULL|                NULL|                NULL|                NULL|               NULL|       NULL|                NULL|         NULL|            NULL|     NULL|                NULL|      NULL|                NULL|               NULL|Someone needs to ...|\n|                NULL|               NULL|                NULL|                NULL|           NULL|                NULL|               NULL|                NULL|                NULL|                NULL|               NULL|       NULL|                NULL|         NULL|            NULL|     NULL|                NULL|      NULL|                NULL|               NULL|#HuntersLapTop #H...|\n| 2019-04-27 15:45:21|     United States |          39.7837304|        -100.4458825|           NULL|       United States|      North America|                NULL|                NULL|2020-10-21 08:14:...|               NULL|       NULL|                NULL|         NULL|            NULL|     NULL|                NULL|      NULL|                NULL|               NULL|All investment in...|\n|1.317990769412739...|               NULL|                NULL|                NULL|           NULL|                NULL|               NULL|                NULL|                NULL|                NULL|               NULL|       NULL|                NULL|         NULL|            NULL|     NULL|                NULL|      NULL|                NULL|2020-10-19 00:47:41|Starting to think...|\n|                 0.0|    Twitter Web App|         567638278.0|              Rick C|   tweetingbrit|UK London expat, ...|2012-04-30 23:02:19|              1293.0|Los Angeles + Coa...|          34.0536909|-118.24276599999999|Los Angeles|United States of ...|North America|      California|       CA|2020-10-21 08:14:...|      NULL|                NULL|               NULL|In public I don't...|\n|1.317990771883114...|               NULL|                NULL|                NULL|           NULL|                NULL|               NULL|                NULL|                NULL|                NULL|               NULL|       NULL|                NULL|         NULL|            NULL|     NULL|                NULL|      NULL|                NULL|2020-10-19 00:47:41|\"THIS AIRS AT 9 P...|\n|                 0.0|    Twitter Web App|1.122164838430646...|      Rise2ShineLife| Rise2ShineLife|Mine Pi Crypto fr...|               NULL|                NULL|                NULL|                NULL|               NULL|       NULL|                NULL|         NULL|            NULL|     NULL|                NULL|      NULL|                NULL|               NULL|     #WakeUpAmerica\"|\n|                NULL|               NULL|                NULL|                NULL|           NULL|                NULL|               NULL|                NULL|                NULL|                NULL|               NULL|       NULL|                NULL|         NULL|            NULL|     NULL|                NULL|      NULL|                NULL|               NULL|If Hunters laptop...|\n|                NULL|               NULL|                NULL|                NULL|           NULL|                NULL|               NULL|                NULL|                NULL|                NULL|               NULL|       NULL|                NULL|         NULL|            NULL|     NULL|                NULL|      NULL|                NULL|               NULL|         #Trump2020 |\n|                 0.0|Twitter for Android|1.312555170098802...|      JB All the Way|Bluecardinal123|Music lover. Clas...|2020-10-04 00:49:07|                 3.0|                NULL|                NULL|               NULL|       NULL|                NULL|         NULL|            NULL|     NULL|2020-10-21 08:14:...|      NULL|                NULL|               NULL|             #trump\"|\n|1.317990746092286...|               NULL|                NULL|                NULL|           NULL|                NULL|               NULL|                NULL|                NULL|                NULL|               NULL|       NULL|                NULL|         NULL|            NULL|     NULL|                NULL|      NULL|                NULL|2020-10-19 00:47:35|Looks like the Go...|\n|1.317990746201460...|               NULL|                NULL|                NULL|           NULL|                NULL|               NULL|                NULL|                NULL|                NULL|               NULL|       NULL|                NULL|         NULL|            NULL|     NULL|                NULL|      NULL|                NULL|2020-10-19 00:47:35|@carmenmom1209 @r...|\n|1.317990758423482...|                0.0|                 0.0|     Twitter Web App|    570387863.0|      solomon grundy|     solomongrundy6|Used to be Republ...| 2012-05-04 00:06:14|              3682.0|              EARTH|       NULL|                NULL|         NULL|            NULL|     NULL|                NULL|      NULL|2020-10-21 08:14:...|2020-10-19 00:47:38|@Carrie71237358 @...|\n|                NULL|               NULL|                NULL|                NULL|           NULL|                NULL|               NULL|                NULL|                NULL|                NULL|               NULL|       NULL|                NULL|         NULL|            NULL|     NULL|                NULL|      NULL|                NULL|               NULL|             HAHAHA!|\n+--------------------+-------------------+--------------------+--------------------+---------------+--------------------+-------------------+--------------------+--------------------+--------------------+-------------------+-----------+--------------------+-------------+----------------+---------+--------------------+----------+--------------------+-------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45c1a6df-024c-4c83-9037-383ad33a5be8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (1930351, 21)\n"
     ]
    }
   ],
   "source": [
    "#calculating how much the data is left after performing basic steps above the count had reduced significantly from 2.3 million to 1.9 million\n",
    "num_rows = df_t.count()\n",
    "num_columns = len(df_t.columns)\n",
    "print(\"DataFrame shape:\", (num_rows, num_columns)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24341c76-3635-4d14-bdf0-7e664a4daca6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c3f227a-c9e2-4536-b287-b1ea281c7a7e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Now cleaning the tweets using the steps below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc1d4391-80b8-44b7-a844-abe9e65158df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Steps:\n",
    "        1. Convert text to lowercase to standardize it.\n",
    "        2. Remove contents in square brackets as they usually contain metadata like music info or citations.\n",
    "        3. Remove URLs to eliminate non-relevant text and potential privacy issues.\n",
    "        4. Remove any HTML or XML tags that may be present.\n",
    "        5. Remove new lines to clean up formatting.\n",
    "        6. Remove words containing numbers to focus only on textual content.\n",
    "        7. Remove mentions, useful in social media text processing to reduce noise.\n",
    "        8. Remove hashtags, useful for cleaning tags from social media texts.\n",
    "        9. Remove retweet artifacts, common in Twitter data.\n",
    "        10. Replace all non-word characters (except spaces) with spaces to clean up special characters or punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5305679f-4c75-478a-bc24-581c506969f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#creating a udf (user defined function) to remove all the special characters and such as hashtags, emojis, numbers, and lowering all chracters\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>+', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r'@[0-9]+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'RT[\\s]+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Registering the function as a UDF\n",
    "clean_udf = udf(clean, StringType())\n",
    "\n",
    "\n",
    "# Applying the UDF to the 'new_tweet' column to create 'cleaned_tweet'\n",
    "df_t = df_t.withColumn(\"cleaned_tweet\", clean_udf(col(\"new_tweet\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af6dc817-81bf-42b8-8b95-601dbd15faec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### At this point, basic text pre-processing has been completed after removing retweets, setting correct datetime format, and all special characters and whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e424194-d767-4076-8e2f-5174c4e89ff8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter out rows where 'tweet' is None or empty (including rows that only have whitespace)\n",
    "df_t = df_t.filter((col(\"cleaned_tweet\").isNotNull()) & (trim(col(\"cleaned_tweet\")) != \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9634e6c-2388-4b58-baf9-4aa20add95cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b8baa25-2eea-4499-a1e1-d186d36984d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### LANGUAGE  DETECTION IS BEING CONDUCTED HERE TO ONLY INCLUDE THOSE TWEETS THAT ARE IN ENGLISH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af3d89f6-0574-4d09-b719-84f1a780bca8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import wordninja #Probabilistically splitting the  concatenated words using NLP based on English Wikipedia uni-gram frequencies.\n",
    "\n",
    "# Defining a UDF for preprocessing\n",
    "def split_words(text):\n",
    "    return ' '.join(wordninja.split(text))\n",
    "\n",
    "split_words_udf = udf(split_words, StringType())\n",
    "\n",
    "# Applying the UDF to DataFrame and creating two new columns processed_tweet and cleaned_weet using the split word udf\n",
    "df_t = df_t.withColumn(\"processed_tweet\", split_words_udf(col(\"cleaned_tweet\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4258ca3-de1a-40f8-b47e-99d439e8305c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "from langdetect import detect, LangDetectException\n",
    "def is_english(text):\n",
    "    # List of words that should always be considered English cause we were seeing some false negatives and thats not what we want \n",
    "    english_words = {'trump', 'biden', 'election', 'election2020', 'politics'}\n",
    "    \n",
    "    # Check if the text is exactly one of the specified words\n",
    "    if text in english_words:\n",
    "        return True\n",
    "    \n",
    "    try:\n",
    "        # Returns True if the detected language is English\n",
    "        return detect(text) == 'en'\n",
    "    except LangDetectException:\n",
    "        # If langdetect cannot understand the text, it assumes that the lang is non-English\n",
    "        return False\n",
    "\n",
    "# Registering the function as a UDF\n",
    "is_english_udf = udf(is_english, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d205ffe8-0865-43ac-a336-570263adb219",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adding a new column 'is_english' that indicates whether each tweet is in English\n",
    "df_t = df_t.withColumn(\"is_english\", is_english_udf(col(\"processed_tweet\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eba4c8b7-c387-404b-9810-c3889cb5fadf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231782d4-01b2-471b-9ae8-a3481d331526",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Counting the avg word length per tweet and some other metrics since these will be required later on to train ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca5358a4-435c-4a4f-afa2-6798483d4101",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------------+\n|tweet_len|word_count|      avg_word_len|\n+---------+----------+------------------+\n|      112|        19| 5.894736842105263|\n|       50|        12| 4.166666666666667|\n|       55|        13| 4.230769230769231|\n|       27|         4|              6.75|\n|       85|        19| 4.473684210526316|\n|        8|         1|               8.0|\n|      112|        23| 4.869565217391305|\n|       63|        13| 4.846153846153846|\n|       40|        12|3.3333333333333335|\n|       49|        15|3.2666666666666666|\n|      246|        47|  5.23404255319149|\n|       14|         2|               7.0|\n|        6|         1|               6.0|\n|       19|         2|               9.5|\n|      179|        41| 4.365853658536586|\n|        4|         2|               2.0|\n|      174|        35|4.9714285714285715|\n|       20|         4|               5.0|\n|       14|         4|               3.5|\n|       89|        16|            5.5625|\n+---------+----------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Performing the necessary transformations to create \"cleaned_tweet\" column\n",
    "df_t = df_t.withColumn(\"cleaned_tweet\", regexp_replace(col(\"cleaned_tweet\"), \"[^a-zA-Z\\\\s]\", \"\"))\n",
    "\n",
    "# Calculating the tweet length\n",
    "df_t = df_t.withColumn(\"tweet_len\", length(col(\"cleaned_tweet\")))\n",
    "\n",
    "# Calculating the  word count\n",
    "df_t = df_t.withColumn(\"word_count\", size(split(col(\"cleaned_tweet\"), \" \")))\n",
    "\n",
    "# Calculating the average word length\n",
    "df_t = df_t.withColumn(\"avg_word_len\", col(\"tweet_len\") / col(\"word_count\"))\n",
    "\n",
    "# Showing the results\n",
    "df_t.select(\"tweet_len\", \"word_count\", \"avg_word_len\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "852900ac-2e32-4d37-b727-eebc4c4949a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tweet_id', 'likes', 'retweet_count', 'source', 'user_id', 'user_name', 'user_screen_name', 'user_description', 'user_join_date', 'user_followers_count', 'user_location', 'lat', 'long', 'city', 'country', 'continent', 'state', 'state_code', 'collected_at', 'new_created_at', 'new_tweet', 'cleaned_tweet', 'processed_tweet', 'is_english', 'tweet_len', 'word_count', 'avg_word_len']\n"
     ]
    }
   ],
   "source": [
    "#checking all columns of the transformed dataframe\n",
    "column_names = df_t.columns\n",
    "print(column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92e6a8d7-8e9b-4af3-b2c7-cfab9c379154",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- tweet_id: string (nullable = true)\n |-- likes: string (nullable = true)\n |-- retweet_count: string (nullable = true)\n |-- source: string (nullable = true)\n |-- user_id: string (nullable = true)\n |-- user_name: string (nullable = true)\n |-- user_screen_name: string (nullable = true)\n |-- user_description: string (nullable = true)\n |-- user_join_date: string (nullable = true)\n |-- user_followers_count: string (nullable = true)\n |-- user_location: string (nullable = true)\n |-- lat: string (nullable = true)\n |-- long: string (nullable = true)\n |-- city: string (nullable = true)\n |-- country: string (nullable = true)\n |-- continent: string (nullable = true)\n |-- state: string (nullable = true)\n |-- state_code: string (nullable = true)\n |-- collected_at: string (nullable = true)\n |-- new_created_at: string (nullable = true)\n |-- new_tweet: string (nullable = true)\n |-- cleaned_tweet: string (nullable = true)\n |-- processed_tweet: string (nullable = true)\n |-- is_english: boolean (nullable = true)\n |-- tweet_len: integer (nullable = true)\n |-- word_count: integer (nullable = false)\n |-- avg_word_len: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filtering the DataFrame to keep only the tweets that are in English \n",
    "df_t = df_t.filter(col(\"is_english\") == True)\n",
    "df_t.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b16551e2-42e2-42d2-983a-a8ecf67b1323",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61de2fce-d1fb-486e-bd55-5d991bad93bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Removing Stopwords from the cleaned_tweets column and creating a new column known as the filtered_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2445038-04e6-4273-b582-4f53570f172c",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|cleaned_tweet                                                                                                                                                                                                                                  |filtered_tweet                                                                                                                                                  |\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|cptsd writer survivortough mentalhealth   i ignore most dms                                                                                                                                                                                    |c ptsd writer survivor tough mental health ignore dms                                                                                                           |\n|now  it s time to see how all that desperate energized thinking stacked up                                                                                                                                                                     |time see desperate energized thinking stacked                                                                                                                   |\n|  what drives the trump presidency                                                                                                                                                                                                             |drives trump presidency                                                                                                                                         |\n|the mental connection of putin  very clever former kgb officer  and donald trump  desperate personality with a maniacal need to be recognized by strong men who wasn t all that bright  was not any manner comfortable                         |mental connection put clever former kgb officer donald trump desperate personality maniacal need recognized strong men wasn bright manner comfortable           |\n|the first time i heard about trump and the russian beauty contests my thought was something along the line of  oh  that can t be good   i suspect it had a few profanities in it  but that was the gist                                        |first time heard trump russian beauty contests thought something along line oh good suspect profanities gist                                                    |\n|cptsd writer survivortough mentalhealth   i ignore most dms                                                                                                                                                                                    |c ptsd writer survivor tough mental health ignore dms                                                                                                           |\n|that is pretty much what my thought process was  along with the other thoughts of ruin and doom that followed                                                                                                                                  |pretty much thought process along thoughts ruin doom followed                                                                                                   |\n|cptsd writer survivortough mentalhealth   i ignore most dms                                                                                                                                                                                    |c ptsd writer survivor tough mental health ignore dms                                                                                                           |\n|where is that video of the bear  hold on                                                                                                                                                                                                       |video bear hold                                                                                                                                                 |\n|hell  bear in the living room                                                                                                                                                                                                                  |hell bear living room                                                                                                                                           |\n|bear in the woods                                                                                                                                                                                                                              |bear woods                                                                                                                                                      |\n|cptsd writer survivortough mentalhealth   i ignore most dms                                                                                                                                                                                    |c ptsd writer survivor tough mental health ignore dms                                                                                                           |\n|episodicreading founding author                                                                                                                                                                                                                |episodic reading founding author                                                                                                                                |\n|i proposed two fundamental reasons for trump seeking the presidency                                                                                                                                                                            |proposed two fundamental reasons trump seeking presidency                                                                                                       |\n|so you can imagine that on election night when the trump strategists brilliantly played the electoral college against the united states i was equally impressed and horrified  this disaster was getting some first rate thinking put into it  |imagine election night trump strategists brilliantly played electoral college united states equally impressed horrified disaster getting first rate thinking put|\n|cptsd writer survivortough mentalhealth   i ignore most dms                                                                                                                                                                                    |c ptsd writer survivor tough mental health ignore dms                                                                                                           |\n|cptsd writer survivortough mentalhealth   i ignore most dms                                                                                                                                                                                    |c ptsd writer survivor tough mental health ignore dms                                                                                                           |\n|why  because no one in the history of the modern presidency has been so reluctant to disclose their financial assets and dealings  added to that we have learned within the past  hours                                                        |one history modern presidency reluctant disclose financial assets dealings added learned within past hours                                                      |\n|what about debt  i ll give this a possible                                                                                                                                                                                                     |debt ll give possible                                                                                                                                           |\n|self driven  aka ego narcissism    driven by his creditors                                                                                                                                                                                     |self driven aka ego narcissism driven creditors                                                                                                                 |\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Split the processed_tweet column into an array of words\n",
    "df_t = df_t.withColumn(\"cleaned_words\", split(col(\"processed_tweet\"), \" \"))\n",
    "\n",
    "# Create StopWordsRemover instance\n",
    "remover = StopWordsRemover(inputCol=\"cleaned_words\", outputCol=\"filtered_words\")\n",
    "\n",
    "# Remove stop words\n",
    "df_t = remover.transform(df_t)\n",
    "\n",
    "# Join the filtered words back into a single string\n",
    "df_t = df_t.withColumn(\"filtered_tweet\", concat_ws(\" \", col(\"filtered_words\")))\n",
    "\n",
    "# Show the DataFrame with filtered tweets\n",
    "df_t.select(\"cleaned_tweet\", \"filtered_tweet\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dabd14d5-1acd-43a0-90b0-f70a9e818822",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['new_tweet', 'cleaned_tweet', 'processed_tweet','cleaned_words','filtered_words', 'source', 'user_name', 'user_screen_name', 'user_description', 'user_join_date', 'lat', 'long', 'user_id', 'tweet_id', 'new_created_at', 'collected_at','city','country','state','state_code','user_followers_count','retweet_count','likes']\n",
    "df_t = df_t.alias(\"df_transformed_3\")\n",
    "df_t = df_t.drop(*columns_to_drop) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abe8efd4-4f2f-420e-a0f5-85fd4c11ad07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## NOW CACHING THE dataframe \"df_transformed_2\" since this will be used for sentimen Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bd9a1e2-15ec-4577-93d2-44c0791e2a40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- user_location: string (nullable = true)\n |-- continent: string (nullable = true)\n |-- is_english: boolean (nullable = true)\n |-- tweet_len: integer (nullable = true)\n |-- word_count: integer (nullable = false)\n |-- avg_word_len: double (nullable = true)\n |-- filtered_tweet: string (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "df_t.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988d4a90-e815-4f19-a946-8ec80980cdd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- user_location: string (nullable = true)\n |-- continent: string (nullable = true)\n |-- is_english: boolean (nullable = true)\n |-- tweet_len: integer (nullable = true)\n |-- word_count: integer (nullable = false)\n |-- avg_word_len: double (nullable = true)\n |-- filtered_tweet: string (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter the DataFrame to keep only the tweets that are in English \n",
    "df_t = df_t.filter(col(\"is_english\") == True)\n",
    "df_t.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17906ead-742f-4bce-8da2-a200bf6a01dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[user_location: string, continent: string, is_english: boolean, tweet_len: int, word_count: int, avg_word_len: double, filtered_tweet: string]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7b17b4d-909b-44d9-914d-f73ecf2c87f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16e0a7ff-c4ff-4f16-8b71-32d13e4d28e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Using Textblob\n",
    "### Creating Polarity and catching sentiments using textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2d43760-362c-40a5-9833-2656a37d1660",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, FloatType\n",
    "def get_subjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "def get_subjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "def get_polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def get_analysis(score):\n",
    "    if score < 0:\n",
    "        return 'negative'\n",
    "    elif score == 0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'positive'\n",
    "\n",
    "# Register as UDFs\n",
    "subjectivity_udf = udf(get_subjectivity, FloatType())\n",
    "polarity_udf = udf(get_polarity, FloatType())\n",
    "analysis_udf = udf(get_analysis, StringType())\n",
    "\n",
    "\n",
    "# Applying UDFs to the DataFrame\n",
    "df_t = df_t.withColumn('subjectivity', subjectivity_udf(col('filtered_tweet')))\n",
    "df_t = df_t.withColumn('polarity', polarity_udf(col('filtered_tweet')))\n",
    "df_t = df_t.withColumn('sentiment', analysis_udf(col('polarity')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2372bc2-1996-4199-b0ef-cec66825efd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this was done to write the df back to the csv format since this is df will be used for training ML models commenting it out here and in the Biden File\n",
    "#df_t.unpersist()\n",
    "#df_t.cache()\n",
    "#df_t.write.csv(\"/FileStore/tables/hashtag_donaldtrump-ML.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c6d5418-5d69-4bc5-81f0-2fa0659adf9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n|sentiment| count|\n+---------+------+\n| positive|328116|\n|  neutral|528764|\n| negative|186143|\n+---------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Finally getting the count of the sentiments, it takes a lot of time to run so commenting out the latest result \n",
    "sentiment_counts = df_t.groupBy(\"sentiment\").count()\n",
    "\n",
    "# Show the results of the sentiments categorized in positive, neutral and negative categories\n",
    "sentiment_counts.show()\n",
    "\n",
    "\n",
    "#+---------+------+\n",
    "#|sentiment| count|\n",
    "#+---------+------+\n",
    "#| positive|328116|\n",
    "#|  neutral|528764|\n",
    "#| negative|186143|\n",
    "#+---------+------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f413491-9283-4b01-a5ea-3174c1f74f62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "TwitterSentimentAnalysis_V1_TRUMP ",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
